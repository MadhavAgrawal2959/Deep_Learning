{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is single output perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(MyDenseLayer,self).__init__()\n",
    "\n",
    "        #initialize weights and biases\n",
    "        self.W=self.add_weight([input_dim,output_dim])\n",
    "        self.b=self.add_weight([1,output_dim])\n",
    "\n",
    "    def call(self, input):\n",
    "\n",
    "        #forward propagate the inputs\n",
    "        z=tf.matmul(inputs,self.W)+self.b  # here we are implementing the equation z=wx+b where w is the weight matrix and b is the bias matrix and inputs is the input matrix i.e. the input to the layer\n",
    "\n",
    "        #feed through a non-linear activation\n",
    "        output=tf.math.sigmoid(z)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "## instead of using the add_weight method, we can use the Dense class\n",
    "\n",
    "import tensorfow as tf\n",
    "\n",
    "layer=tf.keras.layers.Dense(units=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi output perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(n),#here n is the number of units in the Hidden layer \n",
    "    tf.keras.layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model=tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(n1),#here n1 is the number of units in the Hidden layer 1\n",
    "    tf.keras.layers.Dense(n2),#here n2 is the number of units in the Hidden layer 2\n",
    "    tf.keras.layers.Dense(n3),#here n3 is the number of units in the Hidden layer 3\n",
    "    .\n",
    "    .   # we can add as many hidden layers as we want\n",
    "    .\n",
    "    tf.keras.layers.Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Binary Cross Entopy Loss   #will i pass the test or not based on the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logistics(y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error Loss #for preedicting a real value output like a grade in class in no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.square(tf.subtract(y,predicted)))\n",
    "loss=tf.keras.losses.MSE(y,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "weights=tf.variable([tf.raandom.normal()])\n",
    "\n",
    "while True: #loop forever\n",
    "    with tf.GradientTape() as g:\n",
    "        loss=compute_Loss(weights)\n",
    "        gradient=g.gradient(loss,weights)\n",
    "\n",
    "    weights=weights-lr*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent Algorith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.optimizer.SGD #stochastic gradient descent\n",
    "tf.keras.optimizer.Adam #adaptive moment estimation\n",
    "tf.keras.optimizer.RMSprop #root mean square propagation\n",
    "tf.keras.optimizer.Adagrad #adaptive gradient\n",
    "tf.keras.optimizer.Adadelta #adaptive learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all togethoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model=tf.keras.Sequential([...])\n",
    "\n",
    "#pick your favorite optimizer\n",
    "optimizer=tf.keras.optimizer.SGD() # stochastic gradient descent but we can use any other optimizer as well\n",
    "\n",
    "while True: #Loop forever\n",
    "\n",
    "    #forward pass through the network\n",
    "    prediction=model(X)\n",
    "\n",
    "    with tf.GradientTape() as Tape:\n",
    "        #compute the loss\n",
    "        loss = computer_Loss(y,prediction)\n",
    "\n",
    "    #update the weights using the graadient\n",
    "    grads=tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization 1.Droupout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the training set some the neurons to 0 that is shut down some of the neurons\n",
    "# typically 50% of the neurons are shut down\n",
    "#Force network to not rely on any 1 neuron\n",
    "#After each and every Iteration, we randomly shut down 50% of the neurons agian after the Backpropagation\n",
    "\n",
    "tf.keras.layers.Dropout(p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization 1.Droupout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is used to prevent overfitting\n",
    "#it stops the model when it is going to overfit"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
